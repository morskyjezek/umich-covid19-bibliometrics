{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UMich Covid19 Bibliometrics Python Tools\n",
    "\n",
    "This notebook explains and runs tools to create and update the University of Michigan Office of Research (UMOR)\n",
    "project to identify and list publications by researchers at the University of Michigan \n",
    "relating to the novel coronavirus of 2019, Covid-19 epidemiology, clinical studies of SARS-CoV-2 and its variants, \n",
    "and the social and economic impacts of the response to the pandemic. \n",
    "\n",
    "The following cells contain the necessary libraries and functions that we used to \n",
    "process bibliographic data (CSV files pulled from bibliographic databases), identify new publications,\n",
    "and create lists of DOIs (database object identifiers), which were used to develop a shared bibliography. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "These cells import necessary libraries and set up variables used throughout the tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import csv\n",
    "import os\n",
    "from pathlib import Path\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: these tools assume the notebook and/or script is located in the `workflow-python` folder.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "cur_dir       = Path('.')\n",
    "inventory_dir = Path('..','combined-CSV-logs')\n",
    "csv_dir       = Path('..','source-data')\n",
    "combined_dir  = Path('..','combined-CSVs')\n",
    "logs_dir      = Path('..','project-logs')\n",
    "dois_dir      = Path('..','new-doi-lists')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter the date for which you are combining the files [in format YYYYMMDD]: 20200530\n"
     ]
    }
   ],
   "source": [
    "combine_date = input('Please enter the date for which you are combining the files [in format YYYYMMDD]:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collate CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_csvs(combine_date):\n",
    "    '''\n",
    "    This function searches the `source-CSVs` directory to identify \n",
    "    CSV files that contain the citations that are pulled from \n",
    "    the Dimensions database. These results represent the search \n",
    "    results created by the database to list publications by U of Michigan\n",
    "    authors relating to the Covid-19 coronavirus (SARS-CoV-2). It combines\n",
    "    these CSVs into a single CSV, creating consistent columns and removing \n",
    "    extra data from Dimensions.\n",
    "    \n",
    "    Requires:\n",
    "      combine_date - a datestring in YYYYMMDD format representing the date of the most recent update\n",
    "      \n",
    "    Outputs:\n",
    "      a full CSV titled `combination-list-py-YYYYMMDD.csv`\n",
    "      an inventory log that lists the CSV files combined\n",
    "      info for the project logs    \n",
    "    '''\n",
    "    \n",
    "    #csv processing setup\n",
    "    csv_count = 0 \n",
    "    #some fields contain more characters than the default limit; this increases the field limit\n",
    "    csv.field_size_limit(150000)\n",
    "    csv_list = csv_dir.glob('Dimensions-Publication-2020*.csv')\n",
    "    \n",
    "    #create headers for csv; current as of May 2021\n",
    "    fields = 'Rank,Publication ID,DOI,PMID,PMCID,Title,Abstract,Acknowledgements,Source title,Anthology title,MeSH terms,Publication Date,PubYear,Volume,Issue,Pagination,Open Access,Publication Type,Authors,Authors (Raw Affiliation),Corresponding Author,Authors Affiliations,Research Organizations - standardized,GRID IDs,Country of Research organization,Funder,Times cited,Recent citations,RCR,FCR,Altmetric,Source Linkout,Dimensions URL,FOR (ANZSRC) Categories,Sustainable Development Goals,Publication Date (print),Publication Date (online),Publication Date'\n",
    "    fieldnames = list()\n",
    "    for name in fields.split(','):\n",
    "        fieldnames.append(name)\n",
    "    \n",
    "    # empty list for combined items (ie, rows)\n",
    "    item_list = list()\n",
    "    \n",
    "    # output files\n",
    "    combined_file_name = 'combination-list-py-' + str(combine_date) + '.csv'\n",
    "    combinelog_file_name = 'combinelog_' + str(combine_date) + '.tsv'\n",
    "    inventory_file_name = 'csv_inventory_' + str(combine_date) + '.tsv'\n",
    "\n",
    "    #for inventory log\n",
    "    lines = 0\n",
    "    inventory_log = 'File \\tNumber of CSVs \\tLines \\t Cumulative Lines\\n'\n",
    "\n",
    "\n",
    "    # collate and combine the Dimensions CSVs\n",
    "    for f in csv_list:\n",
    "        if f.is_file() == True:\n",
    "            print('Collating',f)\n",
    "            csv_count += 1\n",
    "            inventory_log += str(f) + ' \\t' + str(csv_count) + '\\t'\n",
    "            with f.open(mode = 'r', newline = '', encoding='utf-8') as bibs:\n",
    "                #skip first line\n",
    "                itemdata = bibs.readlines()[1:] # skip the header information\n",
    "                linecount = len(itemdata)\n",
    "                lines = lines + linecount\n",
    "                print(linecount,'lines;',lines,'cumulative lines')\n",
    "                inventory_log += str(linecount) + ' \\t' + str(lines) + '\\n'\n",
    "                items = csv.DictReader(itemdata, delimiter=',', restval='Blank')\n",
    "                for item in items:\n",
    "                    item_list.append(item)\n",
    "        else:\n",
    "            print('Not a file:',str(f))\n",
    "    print('Collated ' + str(csv_count) + ' csv files and ' + str(lines) + ' lines on ' + str(combine_date)+'.')\n",
    "    total_items = len(item_list)\n",
    "\n",
    "    # write the CSVs to a single file\n",
    "    with open(Path(combined_dir,combined_file_name), 'w') as f:\n",
    "        combination_file = csv.DictWriter(f, fieldnames=fieldnames, restval='Blank', extrasaction='ignore') # use restval to add where no values are recorded and extrasaction set to ignore errors when fieldsets do not match\n",
    "        combination_file.writeheader()\n",
    "        combination_file.writerows(item_list)\n",
    "\n",
    "    print('Combined csv files into: ' + str(combined_dir) + '/' + str(combined_file_name))\n",
    "    \n",
    "    #create CSV inventory for this update\n",
    "    with open(Path(inventory_dir,inventory_file_name), 'w', encoding='utf-8') as logfile:\n",
    "        logfile.write(inventory_log)\n",
    "        print('Logged CSV list in: ' + str(inventory_dir) + '/' + str(inventory_file_name))\n",
    "    \n",
    "    # store collation numbers\n",
    "    collate_csv_report = {\n",
    "        'combine_date': combine_date,\n",
    "        'csv_count'   : csv_count,\n",
    "        'total_items' : len(item_list), \n",
    "        'combined_file_name': str(Path(combined_dir,combined_file_name)),\n",
    "        'inventory_file_name': str(Path(inventory_dir,inventory_file_name)),\n",
    "    }\n",
    "    \n",
    "    return collate_csv_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the functions to make updates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collating ../source-data/Dimensions-Publication-2020-05-18_13-12-59.csv\n",
      "5 lines; 5 cumulative lines\n",
      "Collating ../source-data/Dimensions-Publication-2020-05-30_19-23-27.csv\n",
      "238 lines; 243 cumulative lines\n",
      "Collating ../source-data/Dimensions-Publication-2020-05-20_03-39-17.csv\n",
      "192 lines; 435 cumulative lines\n",
      "Collating ../source-data/Dimensions-Publication-2020-04-29_23-36-00.csv\n",
      "115 lines; 550 cumulative lines\n",
      "Collating ../source-data/Dimensions-Publication-2020-05-11_16-12-08.csv\n",
      "170 lines; 720 cumulative lines\n",
      "Collated 5 csv files and 720 lines on 20200530.\n",
      "Combined csv files into: ../combined-CSVs/combination-list-py-20200530.csv\n",
      "Logged CSV list in: ../combined-CSV-logs/csv_inventory_20200530.tsv\n"
     ]
    }
   ],
   "source": [
    "csv_update = collate_csvs(combine_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'combine_date': '20200530', 'csv_count': 5, 'total_items': 715, 'combined_file_name': '../combined-CSVs/combination-list-py-20200530.csv', 'inventory_file_name': '../combined-CSV-logs/csv_inventory_20200530.tsv'}\n"
     ]
    }
   ],
   "source": [
    "print(csv_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine and identify unique DOIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_new_dois(combine_date):\n",
    "    '''\n",
    "    This function reads in data from the most recent combined list of articles,\n",
    "    pulls the DOIs (database object identifiers), which are the most consistent\n",
    "    identifiers for articles and can be reused in bibliographic citation software,\n",
    "    then combines them, identifies how many unique dois are in the list, and \n",
    "    identifies new DOIs that did not appear in previous data pulls\n",
    "\n",
    "    Requires:\n",
    "      combine_date - a datestring in YYYYMMDD format representing the date of the most recent update\n",
    "\n",
    "    Output:\n",
    "      writes a text file with the new DOI list\n",
    "      returns numbers for the project and update logs \n",
    "    '''\n",
    "    \n",
    "    # DOI processing setup\n",
    "    combined_dois = list()\n",
    "    doi_key_errors_c = 0 \n",
    "    doi_blanks_c = 0\n",
    "    #doi_counts = dict()\n",
    "    unique_dois = 0\n",
    "    new_dois = list()\n",
    "    new_doi_c = 0\n",
    "    \n",
    "    # location for input CSV with items\n",
    "    combined_csv_loc = Path(combined_dir,'combination-list-py-' + combine_date + '.csv')\n",
    "\n",
    "    # list for items (ie, CSV rows)\n",
    "    item_list = list()\n",
    "    \n",
    "    # output files\n",
    "    doioutput_file_name = 'unique_doi_output-' + str(combine_date) + '.txt'\n",
    "    \n",
    "    # create combined_dois list\n",
    "    with open(combined_csv_loc, 'r', encoding='utf-8') as f:\n",
    "        item_list = csv.DictReader(f)\n",
    "        for item in item_list:\n",
    "            item_doi = item.get('DOI')\n",
    "            try:\n",
    "                if item_doi == '':\n",
    "                    #print('DOI is blank:', item['Title'])\n",
    "                    doi_blanks_c += 1\n",
    "                elif item_doi == 'DOI':\n",
    "                    #print('row is header')\n",
    "                    continue\n",
    "                elif item_doi.startswith('10.') == False:\n",
    "                    print('Invalid DOI:',item_doi)\n",
    "                else:\n",
    "                    combined_dois.append(item_doi)\n",
    "            except KeyError:\n",
    "                doi_key_errors_c += 1\n",
    "    total_doi_c = len(combined_dois)\n",
    "    print('Items with no DOI value or empty string:',doi_blanks_c,'\\nKey Errors:',doi_key_errors_c,'\\n\\nCollated',str(total_doi_c),'DOIs; some may be duplicates.')\n",
    "    \n",
    "    # create unique DOI list\n",
    "    # count the values and identify single occurence DOIs\n",
    "    c = Counter(combined_dois)\n",
    "    c.most_common\n",
    "    doi_counter = dict(c)\n",
    "    unique_dois_c = len(doi_counter)\n",
    "\n",
    "    # isolate the unique DOIs (that is, the \"new\" ones that only occur once in the list)\n",
    "    for k,v in doi_counter.items():\n",
    "        if v < 2:\n",
    "            new_dois.append(k)\n",
    "    new_doi_c = len(new_dois)\n",
    "    print('Identified',str(new_doi_c),'new DOIs (single occurences).\\n')\n",
    "    \n",
    "    # write unique dois to file\n",
    "    # write new DOIs to file\n",
    "    line_c = 0\n",
    "\n",
    "    with open(Path(dois_dir,doioutput_file_name), 'w') as fh:\n",
    "        for doi in new_dois:\n",
    "            line = str(doi) + '\\n'\n",
    "            fh.write(line)\n",
    "            line_c += 1\n",
    "        print(f'Wrote unique dois to file: {doioutput_file_name} ({str(line_c)} lines)')\n",
    "    \n",
    "    # store update report\n",
    "    unique_doi_report = {\n",
    "        'combine_date'  : combine_date,\n",
    "        'blank_dois'    : doi_blanks_c,\n",
    "        'total_doi_c'   : total_doi_c,\n",
    "        'unique_dois_c' : unique_dois_c,\n",
    "        'new_doi_c'     : new_doi_c,\n",
    "        'doioutput_file_name': str(Path(dois_dir,doioutput_file_name)),\n",
    "    }\n",
    "    \n",
    "    return unique_doi_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items with no DOI value or empty string: 0 \n",
      "Key Errors: 0 \n",
      "\n",
      "Collated 715 DOIs; some may be duplicates.\n",
      "Identified 46 new DOIs (single occurences).\n",
      "\n",
      "Wrote unique dois to file: unique_doi_output-20200530.txt (46 lines)\n"
     ]
    }
   ],
   "source": [
    "doi_update = identify_new_dois(combine_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'blank_dois': 0, 'total_doi_c': 715, 'unique_dois_c': 236, 'new_doi_c': 46, 'doioutput_file_name': '../new-doi-lists/unique_doi_output-20200530.txt'}\n"
     ]
    }
   ],
   "source": [
    "print(doi_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logging\n",
    "\n",
    "Update the project log file with information about the current update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project log update def write_project_logs(combine_date)\n",
    "def update_project_log(combine_date, csv_update, doi_update):\n",
    "    '''\n",
    "    This function should be run after the CSVs are compiled and unique DOIs are identified.\n",
    "    \n",
    "    This function will update the project log to record the update as of the combine_date.\n",
    "\n",
    "    Inputs:\n",
    "      combine_date should be a date formatted YYYYMMDD\n",
    "      csv_update should be a dictionary generated by the collate_csvs() function\n",
    "      doi_update should be a dictionary generated by the identify_new_dois() function\n",
    "      \n",
    "    Outputs:\n",
    "      append a new line to the `project_log.tsv` that shows information about the current update\n",
    "    '''\n",
    "\n",
    "    # log file name\n",
    "    projectlog_file_name = 'project_log.tsv'\n",
    "\n",
    "    # prepare the log information\n",
    "    project_update_entry = str(combine_date) + '\\t' + str(csv_update['csv_count']) + '\\t' + str(csv_update['total_items']) + '\\t' + str(doi_update['total_doi_c']) + '\\t' + str(doi_update['unique_dois_c']) + '\\t' + str(doi_update['new_doi_c']) + '\\tNo note recorded\\t' + str(csv_update['combined_file_name']) + '\\t' + str(csv_update['inventory_file_name']) + '\\t' + str(doi_update['doioutput_file_name'])\n",
    "\n",
    "    with open(Path(logs_dir,projectlog_file_name), mode='a', newline='') as logfile:\n",
    "        logfile.write('\\n')\n",
    "        logfile.write(project_update_entry)\n",
    "        print('Updated project log file: project_log.tsv\\nIf you need to record a note about this update, open the log and enter it.')\n",
    "\n",
    "    # create a separate log file for the  current update? \n",
    "    #logfile.write(projectlog_file_name)\n",
    "    #logfile.write('projectlog.tsv')\n",
    "    print('\\nUpdated project log for',str(combine_date),'update.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated project log file: project_log.tsv\n",
      "If you need to record a note about this update, open the log and enter it.\n",
      "\n",
      "Updated project log for 20200530 update.\n"
     ]
    }
   ],
   "source": [
    "update_project_log(combine_date, csv_update, doi_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Log file\n",
    "\n",
    "Run the following cell to create a new project log file template, if needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_log(log_name):\n",
    "    '''\n",
    "    Writes logs for the current update and adds information to a new, overall project log file.\n",
    "    \n",
    "    Warning: running this script may overwrite project log files, use with caution.\n",
    "    \n",
    "    Requires:\n",
    "      combine_date - a datestring in YYYYMMDD format representing the date of the most recent update\n",
    "    '''\n",
    "\n",
    "    log_name = str(log_name) + '.tsv'\n",
    "    \n",
    "    headers = 'Date_of_update \\tCollated CSV files \\tNumber of items/articles in update \\tNumber of DOIs in update \\tUnique DOIs in update (one or more occurrence) \\tNew DOIs in update (only one occurence) \\tNote \\tCombined CSV file \\t CSV Inventory file \\tNew DOI file'\n",
    "#    sample_project_update = 'test update'\n",
    "\n",
    "    with open(Path('..',log_name), mode='w', encoding='utf-8') as new_log:\n",
    "        new_log.write(headers + '\\n')\n",
    "    print('Wrote new log file:', log_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_log('project_log')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
